---
title: "13 strojno ucenje - linearna regresija"
author: ""
date: ""
output:
  html_document: default
---

```{r setup, include = F}
library(MASS)
library(tidyverse)
library(stringr)
library(ggplot2)
library(GGally)
library(sn)
library(gridExtra)
library(Hmisc)
library(broom)
library(car)
library(corrplot)

knitr::opts_chunk$set(results = 'hold')
```



## Strojno učenje i prediktivna analiza

*Strojno učenje*: polje računalne znanosti koje se bavi specifičnim načinom programiranja u kojem računalu ne dajemo eksplicitne instrukcije, već očekujemo da računalo samostalno dođe do određenih spoznaja na osnovu odabranih podatkovnih skupova i određene metode "učenja"

Često se dijeli na:

   - "nadzirano učenje" (engl. *supervised learning*), gdje imamo jasno definirane ulaze i izlaze tj. ciljeve, te      - "nenadzirano učenje" (engl. *unsupervised learning*), gdje nemamo unaprijed definirane izlaze već očekujemo da će računalo analizirajući samo ulaze doći do nekih korisnih spoznaja o samim podacima. 
   
Postoje i dodatne discipline strojnog učenja kao što su "poticano učenje" (engl. *reinforced learning*), detekcija anomalija (engl. *anomaly detection*) itd. koje nećemo dodatno objašnjavati.

## Dubinska analiza i otkrivanje znanja

Pojam strojnog učenja često se povezuje sa pojmom "dubinske analize podataka" (engl. *data mining*) i "oktrivanja znanja iz podatkovnih skupova" (engl. *KDD - knowledge discovery from data*). 

Proces dubinske analize, tj. otkrivanja znanja koji koristi realne podatke je često opsežan, sveobuhvatan proces koji podrazumijeva dohvat, čišćenje i transformaciju podataka, eksploratornu analizu, razdvajanje skupa podataka na zasebne skupove od kojih će se jedni koristiti za svaranje prediktivnih i deskriptivnih modela a drugi za provjeru njihove učinkovitosti, isprobavanje i evaluaciju različitih metoda, stvaranje konačnih modela, pisanje izvještaja te integraciju i dokumentiranje modela u postojeće informatičke sustave. 

U ovoj lekciji mi ćemo se usredotočiti na prikaz jedne vrlo često korištene metode strojnog učenja - **linearne regresije** - pri čemu ćemo naglasak dati na njezino korištenje u sklopu programskog jezika R. 


## Linearna regresija

Linearna regresija je iznimno popularna metoda strojnog učenja koja uz pomoć matematičkih i statističkih temelja opisuje potencijalnu linearnu povezanost varijabli podatkovnog skupa. Ukoliko se pronađe dovoljno dokaza da linearna povezanost postoji, onda smo dobili potencijalno koristan uvid u stvarni odnos dvaju varijabli skupa. Isto tako, dobivena "formula" se može koristiti da se na osnovu poznate vrijednosti jedne varijable (prediktora) s određenom sigurnosti procijeni vrijednost druge (ciljne) varijable. 

Jednostavna linearna regresija počiva na matematičkim temeljima. Najčešće korištena metoda određivanja koeficijenta smjera i odsječka jest **metoda najmanjih kvadrata**, koja povlači pravac između točaka grafa tako da je suma kvadrata udaljenosti točaka od pravca (tzv. reziduali) minimalna. Matematičkim postupkom možemo izvesti formulu koja će egzaktno izračunati navedene parametre, pod uvjetom da imamo dovoljno obzervacija.

Iako danas postoje puno naprednije prediktivne metode, linearna regresija se i dalje često koristi kako zbog svoje jednostavnosti, tako i zbog činjenice razumijevanje linearne regresije postavlja čvrste temelje za učenje i razumijevanje naprednijih metoda. Zbog svega toga, linearna regresija predstavlja logičan i vrlo važan prvi korak za ulazak u svijet prediktivne analize.


## Jednostavna linearna regresija

```{r, echo = F}
set.seed(1234)
x <- 1:100
df <- data.frame(x = x + rnorm(100),
                 y1 = 4 * x + 20 + rnorm(100, 25, 25),
                 y2 = 4 * x + 20 + rnorm(100, 25, 120),
                 y3 = -(x-50)^2/6 + 400 + rnorm(100, 25, 75), 
                 y4 = 0.1 * x + 20 + rnorm(100, 25, 100))
df <- sample_n(df, 100)
write.csv(df, file = "linreg_podaci.csv", row.names = F)

```

Jednostavna linearna regresija je **metoda nadziranog strojnog učenja za predviđenje ciljne numeričke varijable uz pomoć linearne funkcije ulazne varijable**. Na ovaj način stvaranje prediktivnog modela svodi se na postupak određivanja **koeficijenta smjera** i **odsječka** koji će tvoriti jednostavnu formulu za izračun ciljne varijable uz pomoć ulaznog parametra. Budući da se ova metoda stvori na pogađanje navedenih parametara, metoda linearne regresije spada u tzv. "parametarske metode" strojnog učenja, tj. prediktivne analize. 

Motivaciju za provođenje jednostavne linearne regresije često nalazimo tijekom procesa eksploratorne analize podataka, poglavito tijekom vizualizacije dvije numeričke varijable točkastim grafovima. Ukoliko je jedna od tih varijabli nama interesantna kao ciljna varijabla prediktivnog modela, a na grafu povijesnih podataka vidimo kako u međudnosu sa nekom drugom varijablu točke tvore približni oblik pravca, onda se tu radi o očitom kandidatu za metodu jednostavne linearne regresije.

## Kolinearnost varijabli podatkovnog okvira

Pogledajmo sljedeći zadatak. U njemu se koristi "umjetni" podatkovni okvir u kojem imamo ulaznu varijablu `x` i četiri moguće ciljne varijable `y1`, `y2`, `y3` i `y4`. Svaka od tih varijabli nastala je određenom transformacijom ulaza uz dodavanje određene količine šuma. Ideja zadatka jest proučiti odnos između ulazne i mogućih izlaznih varijabli te uočiti koji od tih odnosa je dobar kandidat za jednostavnu linearnu regresiju.



## ZADATAK 13.1 - uočavanje linearne povezanosti varijabli


```{r}
# u varijablu `df` učitajte podatke iz datoteke `podaci1.csv`
# proučite učitani podatkovni okvir
head(df)
summary(df)

# nacrtajte točkaste grafove odnosa varijable 
# x sa svakom pojedinom varijablom y iz gornjeg podatkovnog okvira
# svakom grafu dodajte i geometriju zaglađivanja, metoda `lm`
g1 <- ggplot(df, aes(x = x, y = y1)) + geom_point() + geom_smooth(method = lm)
g2 <- ggplot(df, aes(x = x, y = y2)) + geom_point() + geom_smooth(method = lm)
g3 <- ggplot(df, aes(x = x, y = y3)) + geom_point() + geom_smooth(method = lm)
g4 <- ggplot(df, aes(x = x, y = y4)) + geom_point() + geom_smooth(method = lm)

grid.arrange(g1, g2, g3, g4, nrow = 2, ncol = 2)

# odgovorite na pitanja:
# na kojim grafovima uočavate moguću linearnu povezanost varijabli?
# koji graf predočava nelinearnu povezanost?
# za koji graf biste mogli reći da su varijable nezavisne?
print("Linearna povezanost: g1, g2")
print("Nelinearna povezanost: g3")
print("Nezavisne varijable: g4")

```


## Pearsonov koeficijent korelacije

koliko želimo numerički opisati snagu linearnog odnosa dvaju varijabli, možemo se koristiti tzv. "Pearson-ovim koeficijentom korelacije".

Pearson-ov koeficijent korelacije poprima vrijednost iz intervala [-1,1] i kod kojeg -1 znači savršenu negativnu korelaciju, 1 savršenu pozitivnu, a 0 da korelacije nema. 

U R-u ovaj koeficijent možemo dobiti uz pomoć funkcije `cor`.



## ZADATAK 13.2 - izračun koeficijenta korelacije


```{r}
# za svaki graf iz prethodnog zadatka izračunajte i ispišite
# koeficijent korelacije između prikazanih varijabl (funkcija `cor`)
cor(x = df$x, df$y1)
cor(x = df$x, df$y2)
cor(x = df$x, df$y3)
cor(x = df$x, df$y4)

cor(df)
```


## Mjera "R-kvadrat"


Dodatna mjera za opisivanje snage linearnog odnosa  - "**R-kvadrat**" (engl. *R squared*). 

Ova veličina može poprimiti vrijednosti od 0 do 1, gdje vrijednost bliska 1 označava gotovo savršenu linearnu vezu dok vrijednost bliska 0 njezine nepostojanje. 

Interpretacija mjere "R kvadrat" je dosta bitna te ju nije naodmet naučiti - ona se definira kao "**količina varijabilnosti koja je objašnjena modelom**". Ovo možemo jednostavno objasniti na sljedeći način - gledamo omjer koliko su točke na grafu "raspršene" oko zamišljenog pravca s obzirom na njihovu "općenitu raspršenost" oko horizontalnog pravca koji bi prolazio njihovom aritmetičkom sredinom. 


## Funkcija `lm`

U jeziku R jednostavne linearne modele stvaramo uz pomoć funkcije `lm`, što je skraćeno od *linear model*. Ova funkcija ima niz parametara, a mi ćemo koristiti najvažnije - statističku formulu i podatkovni skup na kojem treniramo:

```{r, eval = F}
lm(formula, data)
```

Pojam formule već smo objasnili u poglavlju o vizualizaciji, budući da su istu kao parametar koristili uvjetni (facetirani) grafovi. Podsjetimo se o čemu se radi - "formula" je zapravo skraćeni zapis notacije gdje želimo reći da lijeva strana formule "ovisi" o desnoj strani formule. Ako želimo trenirati linearnu regresiju ciljne varijable `y` u ovisnosti o varijabli `x` za podatkovni okvir `df` i konačni model spremiti u varijablu `linMod`, onda to u programskom kodu izgleda ovako:

```{r,eval = F}
linMod <- lm(y ~ x, data = df)
```


Pokušajmo ovo izvesti samostalno u sljedećem zadatku.




## ZADATAK 13.3 - stvaranje jednostavnog linearnog modela

```{r}
# uz pomoć funkcije `lm` stvorite linearni model podataka iz tablice `df`
# gdje je `x` ulazna a `y1` izlazna varijabla
# rezultat spremite u varijablu `linMod`
linMod <- lm(y1 ~ x, data = df)

# ispišite varijablu `linMod`
linMod
```


## Interpretacija linearnog modela

Ispis varijable `linMod` nam ispisuje formulu koja je korištena za stvaranje modela te izračunate parametre - koeficijent smjera i odsječak (koji nam u pravilu nije interesantan). Koeficijent interpretiramo na sljedeći način - **za pomak od jedne jedinice ulazne varijable, izlazna se mijenja za iznos koeficijenta**.

## Objekt klase `lm`

Ovaj objekt sadržava ne samo koeficijente, već i bogati skup informacija vezanih uz stvoreni linearni model, što uključuje čak i sam podatkovni skup pomoću kojeg je model stvoren. 

Nad ovim objektom možemo izvesti sljedeće funkcije:

- `coef` - vraća koeficijente u obliku vektora
- `fitted.values` - vraća vektor predikcijavdobiven primjenom modela na skup za treniranje
- `residuals` - vraća vektor grešaka dobiven primjenom modela na skup za treniranje
- `summary` - daje sažetak najvažnijih informacija o modelu

Isprobajmo funkciju sažetka - `summary` - nad našim linearnim modelom.



## ZADATAK 13.4 -  sažetak linearnog modela

```{r}
# izvršite funkciju `summary` nad linearnim modelom `linMod`
summary(linMod)
```


## Interpretacija sažetka linearnog modela

Postoji nekoliko različitih "nesigurnosti" u dobiveni rezultat:

1) Da li linearni trend uopće postoji, ili se uočena kolinearnost mogla pojaviti slučajno? 

2) Ako trend postoji, koliko smo sigurni da izračunati koeficijent smjera odgovara "stvarnom"?

3) Konačno, ako trend postoji a mi smo uspjeli dovoljno dobro pogoditi "pravi" koeficijent, koliko nam dodatni "šum" utječe na točnost predikcija? 

Prikazani sažetak nam pruža odgovore na ova pitanja. 

## Stvaranje (novih) predikcija

R nam nudi generičku metodu `predict` kojoj u općenitom slučaju kao parametre šaljemo stvoreni prediktivni model i **podatkovni okvir** sa novim podacima, pri čemu moramo voditi računa da podatkovni okvir ima stupce koji odgovaraju očekivanim ulazima modela. 

Budući da prediktivni modeli često u sebi sadrže i skup korišten za stvaranje modela, ovoj funkciji možemo proslijediti i model bez dodatnih podataka - u tom slučaju ona će nam jednostavno vratiti skup predikacija dobivenih nad originalnim skupom (tj. isti rezultat koji bi nam dala funkcija `fitted.values`).



## ZADATAK 13.5 - stvaranje novih predikcija

```{r}
# sljedeći vektor prikazuje "nove" vrijednosti ulazne varijable `x`
novi_x <- c(-5, 10, 50, 102)

# stvorite i ispišite predikcije za gornji vektor pomoću 
# funkcije `predict` i linearnog modela `linMod`inearnog modela `linMod`
# pripazite da nove podatke šaljete u obliku podatkovnog okvira
predict(linMod, data.frame(x = novi_x))


# izračunajte predikcije "ručno", korištenjem jednadžbe pravca
# i dobivenih koeficijenata linearnog modela
coef(linMod)[1] + novi_x * coef(linMod)[2]


```



## Vizualizacija reziduala

dva pitanja koje analitičar potencijalno može postaviti vezano uz reziduale su:

- da li postoje očiti uzorci u ponašanju reziduala obzirom na slijed originalnih obzervacija? 
- da li reziduali imaju normalnu razdiobu?

## Paket `broom`

Paket `broom` nudi niz funkcija za uređivanje dobivenih prediktivnih modela i lako izvlačenje informacija iz istih - npr. funkcija `tidy` nam daje rezultate modela (tj. koeficijente) složene u lako čitljiv podatkovni okvir, dok nam funkcija `glance` radi isto ali nad parametrima koji opisuju kvalitetu modela. 

Mi ćemo se u nastavku poslužiti metodom `augment` koja na osnovu prosljeđenog prediktivnog modela vraća originalni podatkovni okvir korišten za stvaranje modela, ali proširen sa nizom korisnih stupaca


## ZADATAK 13.6 - paket `broom`

```{r}
# primjenite funkciju `augment` nad linearnim modelom `linMod`
# rezultantni podatkovni okvir pohranite u varijablu `predikcije`

# proučite prvih nekoliko redaka okvira `predikcije`

```



## Funkcija `augment`

Uočite da nam metoda `augment` zapravo proširuje originalni podatkovni okvir nizom stupaca relevantnih za dobiveni linearni model. Ovdje nećemo objašnjavati sve dobivene stupce, no neki od njih su:

- `.fitted` - predikcije dobivene iz modela
- `.se.fit` - standardna greška pojedine predikcije
- `.resid` - iznos reziduala, tj. greške
- `.std.resid` - reziduali standardizirani na interval [0,1]
- `.hat` - mjera "ekstremnosti" ulazne varijable ove obzervacije (engl. *leverage*)
- `.cooksd` - mjera "utjecajnosti" obzervacije (engl. *influential point*); radi se o obzervacijama koju imaju visoku "*leverage*" mjeru i visoki rezidual

Metodu `augment` možemo koristiti i kao alternativu generičkoj metodi `predict` - samo joj moramo proslijediti nove podatke uz pomoć parametra `newdata`.

## Analiza reziduala

Sada kada imamo podatkovni okvir koji sadrži podatke o rezidualima, možemo stvoriti spomenute vizualizacije. Konkretno, stvoriti ćemo

- točkasti grafa sa predikcijama na osi `x` i (standardiziranim) rezidualima na osi `y`
- graf funkcije gustoće razdiobe standardiziranih reziduala
- kvantil-kvantil graf standardiziranih reziduala

Razlog zašto radimo sa standardiziranim umesto "pravim" rezidualima jest samo lakša interpretacija, tj. jednostavnija usporedba rezultata sa "standardnom" normalnom razdiobom koja ima sredinu 0 i standardnu devijaciju 1.



## ZADATAK 13.7 - provjera "normalnosti" reziduala

```{r}
# uz pomoć podatkovnog okvira `predikcije`
# stvorite točkasti graf predikcija i std. reziduala
# na grafu nacrtajte i horizontalnu liniju koja prolazi kroz nulu

# stvorite graf gustoće razdiobe standardnih reziduala
# koristite geometriju `geom_density`

# stvorite kvantil-kvantil graf std. reziduala
# koristite geometriju `geom_qq`
# reziduale postavite na estetiku `sample` (ne `x`!)

```



## Dijagnostika problema uz pomoć vizualizacije reziduala

Neki od mogućih zaključaka nakon stvaranja vizualizacija reziduala mogu biti sljedeći:

- ako točkasti graf sa predikcijama i rezidualima pokazuje očite uzorke, moguće da linearni model nije dobar za opis odnosa prediktora i cilja te treba posegnuti za modelom koji može opisati složeniju prirodu odnosa
- ako graf reziduala ima oblik "lijevka", tj. ako reziduali rastu sa povećanjem vrijednosti predikcija, možda je potrebno transformirati ulazne i/ili izlazne varijable, npr. uz pomoć funkcije korijena ili logaritma
- ako uočavamo neke vrijednosti koje izrazito "iskaču" u grafu reziduala trebamo ih pažljivije pogledati i potencijalno ukloniti iz skupa za stvaranje modela


## Linearna regresija i kategorijske varijable


```{r, echo = F}
set.seed(1234)
x <- c(rep('A', 48), rep('B', 52))
df <- data.frame(x = x,
                 y = 50*(x=='B') + rnorm(100, 25, 10))
df <- sample_n(df, 100)
write.csv(df, file = "podaci2.csv", row.names = F)
```

Može li kategorijska varijabla biti ulaz u prediktivni model?

Može, uz određenu prilagodbu. Moramo **pretvoriti kategorijsku varijablu u binarnu (indikatorsku) varijablu** koja opisuje pripada li određena obzervacija odabranoj kategoriji (ako ne pripada, onda logično pripada onoj drugoj, referentnoj ili *baseline* kategoriji). 

Linearna regresija će potom odrediti koeficijent koji će definirati pravac na način da se koeficijent pribraja ako je indikatorska varijabla `1`, ili se ne uzima u obzir ako je indikatorska varijabla `0`.

## Dvorazinske i višerazinske kategorijske varijable

Koliko nam treba indikatorskih varijabli za kategorijsku varijablu sa dvije kategorije? 

- **jedna** (druga bi bila inverz prve)

Koliko nam treba indikatorskih varijabli za kategorijsku varijablu sa više od dvije kategorije? 

- **jedna manje od broja kategorija**, budući da "nepripadanje" svim kategorijama osim jedne nužno označava pripadanje toj jednoj, preostaloj kategoriji.


## ZADATAK 13.8  - podatkovni okvir sa kategorijskim prediktorom


```{r}
# u varijablu `df` učitajte podatke iz datoteke `podaci2.csv`
# proučite učitani podatkovni okvir
df <- read.csv("podaci2.csv")
head(df)
# nacrtajte točkasti graf ovisnosti varijable `y` o varijabli `x`
ggplot(df, aes(x = x, y = y)) + geom_point()
```



## Automatsko stvaranje indikatorske varijable

Jezik R, tj. funkcija `lm` će **automatski stvoriti** indikatorske varijable ako kategorijske varijable postavimo u regresijsku formulu.

OPREZ! Kod stvaranja predikcija za nove podatke moramo biti sigurni da kategorijska varijabla ne sadrži kategorije koje nisu bile zastupljene u podacima korištenim za stvaranje modela.



## ZADATAK 13.9 - stvaranje linearnog modela sa kategorijskim ulazom


```{r}
# uz pomoć funkcije `lm` stvorite linearni model podataka iz tablice `df`
# gdje je `x` ulazna a `y` izlazna varijabla
# rezultat spremite u varijablu `linMod`
linMod <- lm(formula = y ~ x, data = df)
summary(linMod)
```



## Interpretacija linearnog modela sa kategorijskim ulazom

Vidimo da je sažetak linearnog modela vrlo sličan već prikazanom sažetku gdje je ulazna varijabla bila numeričkog tipa. Razlika u interpretaciji je sljedeća - koeficijent smjera veže se uz konkretnu kategoriju (navedenu uz ime varijable), a tiče se **očekivane razlike u iznosu ciljne varijable kad obzervacija ima navedenu kategoriju, u odnosu na referentnu kategoriju**. 

Za kraj ovog dijela naglasimo samo da je kod korištenja kategorijskih varijabli kao ulaze u linearni model bitno voditi računa o zastupljenosti kategorija, tj. da nemamo **kategorije koje su vrlo slabo zastupljene** u podatkovnom skupu za treniranje. Razlog je taj što ovakve obzervacije vrlo često **imaju veliki utjecaj na regresijski pravac**, a što može imati nepovoljne posljedice na kvalitetu linearnog modela.



## Višestruka (multipla) linearna regresija 


Princip jednostavne linearne regresije lako se proširuje na scenarij kada imamo više ulaznih varijabli - jednostavno rečeno, tražimo funkciju koja će ciljnu varijablu izraziti kao linearnu kombinaciju ulaznih varijabli. Problem izgradnje modela opet se svodi na traženje "dobrih" koeficijenata smjera koji će ići uz svaku ulaznu varijablu (plus odsječak), iako formalno sada ne možemo pričati o "pravcu" regresije već se radi o nešto kompleksnijem pojmu "hiper-ravnine".

## Formule za višestruku linearnu regresiju

Kod višestruke linearne regresije pojavljuje se niz dodatnih izazova s kojima se moramo suočiti, no za samo treniranje modela koristimo već upoznatu funkciju `lm`, kojoj je dovoljno proslijediti željenu formulu, npr:

```{r, eval = F}
y ~ x1 + x2              # `y` kao linearna kombinacija `x1` i `x2`
y ~ .                    # `y` kao linearna kombinacija svih ostalih varijabli
y ~ . - x1 - x2          # `y` kao linearna kombinacija svih ostalih varijabli OSIM x1 i x2
log(y)  ~ x1 + log(x2)   #  prirodni logaritam od `y` kao linearna kombinacija `x1` i
                              # prirodnog logaritma od `x2`
y ~ x1 + I(x2^2)         # `y` kao linearna kombinacija `x1` i kvadrata od `x2`
```



## Podatkovni skup `mtcars`

Pokušajmo sada stvoriti prediktivni model sa više ulaznih varijabli. U zadatku ćemo koristiti otprije upoznati podatkovni skup `mtcars` (ako je potrebno podsjetite se dodatnih detalja o ovom skupu uz pomoć dokumentacije).

```{r, warnings = F}
data(mtcars)
# faktoriziramo stupce  `vs` i `am`
cols <- c("vs", "am")
mtcars[, cols] <- lapply(mtcars[, cols], factor)

glimpse(mtcars)
```



## ZADATAK 13.10- stvaranje linearnog modela sa više prediktora

```{r}
# uz pomoć funkcije `lm` stvorite linearni model podataka iz tablice `mtcars`
# koristite varijable `am`,  `cyl` i `wt` kao ulaz
# i varijablu `mpg` kao izlay
linMod <- lm(mpg ~ am + cyl + wt, mtcars)

# proučite sažetak modela
summary(linMod)
```


## Kolinearnost ulaznih varijabli

Vratimo se navedenoj tvrdnji da kopeficijenti višestruke linearne regresije tretiraju ulazne varijable kao da su nezavisne jedna od druge. To relativno često nije slučaj tj. ulazne varijable nisu samo kolinerne sa ciljem, nego i između sebe. Ovo je poznati problem *kolinearnosti ulaznih varijabli*.

Ako možda nije odmah jasno zašto je ovo problematično, dovoljno je ponovo pogledati interpretaciju koeficijenata višestruke linearne regresije. Pretpostavka da se jedan ulazni parametar mijenja dok njemu kolinearan ostaje fiksan je nerealan, što se odražava u podatkovnom skupu a samim time i u porastu "nesigurnosti" linearnog modela. Zbog toga u konačnom rezultatu modela možemo dobiti **veće p-vrijednosti ulaznih varijabli**, tj. one mogu biti tretirane kao irelevantne, iako su zapravo snažno linearno povezane s ciljem.

Kolinearnost ulaznih varijabli možemo provjeriti uz pomoć već korištene funkcije `cor`, koja će nam kao rezultat dati korelacijsku matricu. Čvrste granice što predstavlja "previsoku" kolinearnost nema, ali vrijednosti koje su po apsolutnom iznosu iznad 0.7 vjerojatno zaslužuju posebnu pažnju. Alternativno, možemo pozvati funkciju `ggpairs` koja će nam dati iznose koeficijenta korelacije, ali i vizualizacije pomoću kojih možemo uočiti linearne trendove između ulaznih varijabli.



## ZADATAK 13.11 - kolinearnost ulaznih varijabli

```{r}
# u podatkovni okvir `mtcarsNumInputs` ubacite sve numeričke
# varijable podatkovnog okvira `mtcars` osim ciljne varijable `mpg`

# uz pomoć funkcije `cor` ispišite korelacijsku matricu
# numeričkih stupaca okvira `mtcarsNumInputs`

# proslijedite taj okvir funkciji `ggpairs` paketa `GGally`



```


Još jedan zgodan način vizualizacije kolinearnosti nam pruža funkcija corrplot istoimenog paketa.

## Zadatak 13.12 - funkcija `corrplot`

```{r}
# učitajte paket `corrplot` (instalirajte ako je potrebno)
# pozovite funkciju `corrplot` kojoj ćete proslijediti korelacijski matricu
# stupaca okvira `mtcarsNumInputs`
```


## Pojam "multikolinearnosti"


Vidimo da neke varijable (npr. `disp` i `wt`, koje predstavljaju volumen motora i težinu vozila) imaju vrlo visoku razinu kolinearnosti, što znači da zahtijevaju posebnu pažnju.

Kolinearnost varijabli koju smo gledali tiče se parova varijablu, no moguća je i jedna interesantna pojava - tzv. *multikolinearnost*. Kod ove pojave moguće je da se **kolinernost očituje tek u kombinaciji tri i više varijabli**, tj. kod gledanja zasebnih parova ne vidimo ništa neobično ali negativni učinako kolinearnosti i dalje postoji.

Kako bi se olakšalo uočavanje ove pojave, razvijena je tzv. VIF mjera (engl. *variance inflation factor*). Ovdje nećemo detaljno objašnjavati značenje i teoriju iza ove mjere, nego samo navesti činjenicu kako ju upotrijebiti u našem linearnom modelu te kako interpretirati dobivene vrijednosti.

VIF mjeru izračunavamo uz pomoć funkcije `vif` koju možemo naći u paketu `car`. Ova funkcija kao parametar očekuje  linearni model kojeg analiziramo. Kao rezultat dobivamo numerički vektor sa VIF vrijednostima svake ulazne varijable. Opet, nema egzaktne granice što predstavlja "visoku" VIF vrijednost - u literaturi nailazimo na različite naputke, a kao granica se spominje vrijednost 5, vrijednost 10 i sl. Kao općenito pravilo možemo zapamtiti da dvoznamenkasti VIF označava "problematičnu" ulaznu varijablu.


## ZADATAK 13.13 - multikolinearnost


```{r}
# istrenirajte linearni model `lm_sve` koja za okvir `mtcars`
# gleda ovisnost varijable `mpg` o svim ostalim varijablama
#
# navedeni model proslijedite funkciji `vif` paketa `cars` i ispišite rezultat
```


## Rješavanje problema kolinearnosti i multikolinearnosti

Sad kada znamo da je kolinearnost ulaznih varijabli potencijalni problem, možemo postaviti pitanje - što učiniti kada uočimo navedenu pojavu? Neke od mogućih rješenja su:

- izbaciti jednu od para problematičnih varijabli
- transformirati kolinearne varijable u alternativnu jedinstvenu ulaznu varijablu


## ZADATAK 13.14 - linearni model sa kolinearnim ulazima

```{r}
# trenirajte sljedeće linearne modele:
#  `lm1` - `mpg` u ovisnosti o `disp`
#  `lm2` - `mpg` u ovisnosti o `wt`
#  `lm3` - `mpg` u ovisnosti o `disp` i `wt`

#
# proučite sažetke dobivenih linearnih modela,
# poglavito t-vrijednosti parametara i prilagođenu R-kvadrat mjeru

```



## Interpretacija dobivenih rezultata


Usporedivši rezultate dobivenih linearnih modela možemo zaključiti kako linearni model `lm3` ima najmanju standardnu grešku reziduala i najveću "R-kvadrat" mjeru te je time najbolja od tri opcije. No potencijalni problem se očituje kada pogledamo p-vrijednosti, koje su obje znatno veće nego kada smo trenirali modele sa svakom varijablom zasebno. Dakle, kolinearnost varijabli ne mora nužno utjecati na prediktivnu moć modela, ali unosi potencijalno veliku nesigurnost u modelu smislu da sve kolinearne prediktore izbacimo iz modela kao irelevantne. To bi se mogao pokazati kao velik problem kada imamo više potencijalnih prediktora i pokušavamo odabrati relevantni podskup, što je tema kojom ćemo se baviti u nastavku.

## Odabir varijabli

**Odabir varijabli (*variable selection*) jedan od ključnih izazova s kojima se suočavamo u izradi prediktivnih modela**, ne samo kod linearne regresije već i općenito.

Očito je da bi dobar model trebao sadržavati ulazne varijable koje dobro "objašnjavaju" ciljnu varijablu a koje su što više međusobno nezavisne. Mogući **kriterij za odluku** koje varijablu odabrati za ugrađivanje u model tako može biti utjecaj na povećanje zajedničke **"R-kvadrat"** mjere, smanjenje **standardne greške reziduala** ili **p-vrijednost** koeficijenta za tu ulaznu varijablu. Pored ovih "standardnih" kriterija postoje i razni drugi, kao npr. popularni **AIC** (engl. *Akaike information criterion*) koji procjenjuje informativnost modela uz penaliziranje većeg broj varijabli.

## Iterativna (*stepwise*) izgradnja prediktivnog modela

Varijable možemo odabirati ručno, no puno je lakše taj posao ostaviti računalu. Statistički alati, uključujući i jezik R, često imaju ugrađene algoritme koji na osnovu zadanog kriterija izgrađuju prediktivni model iterativnim odabirom varijabli. Najčešće strategije izgradnje modela su:

- **"unatrag" od potpunog modela**, npr. iterativno se izbacuju varijable sa najvećom p-vrijednosti
- **"unaprijed" od praznog modela**, npr. iterativno se dodaju varijable koje najviše smanjuju RMSE
- razne **hibridne** metode


## Funkcija `stepAIC`

Jezik R ima funkciju `step` za iterativno (engl. *stepwise*) stvaranje prediktivnih modela, no u praksi se preporučuje puno bolja funkcija `stepAIC` koju možemo naći u paketu `MASS`. Ova funkcija između ostalog očekuje sljedeće parametre:

- `object` - inicijalni (linearni) model 
- `scope` - raspon modela koje uključujemo u strategiju; potreban je samo za izgradnju "unaprijed" a prosljeđujemo joj listu sa "najsiromašnijim" (`lower`) i  "najbogatijim" (`upper`) modelom
- `direction` - inaprijed (`forward`), unatrag (`backward`) ili hibridno (`both`)
- `trace` - binarna varijabla koja opisuje želimo li ispis cijelog procesa odabira varijabli


Za kraj ćemo iterativno stvoriti prediktivni model za podatkovni okvir `mtcars` gdje će opet ciljna varijabla biti potrošnja (varijabla `mpg`) dok će kandidati za ulaznu varijablu biti sve ostale varijable.




## Zadatak 13.15 - iterativna selekcija varijabli za linearnu regresiju

```{r}
library(MASS) # ako je potrebno

# stvaramo "potpuni" i "prazni" model 
lm_sve <- lm(mpg ~ ., data = mtcars)   
lm_prazan <- lm(mpg ~ 1, data = mtcars)

# pogledajte sažetke gornjih modela kako bi 
# dobili dojam kako rade "ekstremi"

print("LM_SVE")
summary(lm_sve)
print("LM_PRAZAN")
summary(lm_prazan)


# uz pomoć funkcije `stepAIC` stvorite modele `lm1` i `lm2` 
# na sljedeći način
# `lm1` - nastaje selekcijom "unatrag" od punog modela
#         (parametar direction = "backward")
# `lm2` - nastaje selekcijom "unaprijed" od praznog modela
#         (parametri direction = "forward" , 
#         scope = list(upper = lm_sve, lower = lm_prazan))
#
# proučite sažetke dobivenih modela

lm1 <- stepAIC(lm_sve, direction = "backward")
summary(lm1)
print("====================================")
print("====================================")
print("====================================")
lm2 <- stepAIC(lm_prazan, direction = "forward", scope = list(upper = lm_sve, lower = lm_prazan))
summary(lm2)
```



## Zaključak

Vidimo da su dvije strategije rezultirale sa dva potpuno različita modela sličnih performansi. To znači da ne možemo očekivati pronalazak optimalnog modela, već samo automatizirani pokušaj pronalaska "najboljeg" uz kriterije i uvjete koje smo inicijalno postavili. Ukoliko nismo zadovoljni rezultatom, uvijek možemo pokušati sa alternativnim parametrima funkcije, drugom strategijom ili inicijalnim skupom ulaznih varijabli. U svakom slučaju, pronalazak zadovoljavajućeg prediktivnog modela nije jednostavan problem, a uvijek je poželjno sjetiti se citata koji se često pronalazi u knjigama o statistici i strojnom učenju, a kojeg je navodno izrekao statističar George Box - "Svi modeli su pogrešni, ali neki su korisni".

